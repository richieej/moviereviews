{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV08F9yp60xV"
      },
      "source": [
        "Movie Review Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_0YJ59N4qRt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f4f566e4-bd96-4dc7-847d-2eb1d9107b84"
      },
      "source": [
        "%tensorflow_version 2.x \n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)\n",
        "#movie data set, data is already preprocessed and labelled with integers. integer number is essentially a rank of frequency\n",
        "#integer 0 is the most common word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoH-h3O65Tap"
      },
      "source": [
        "#cannot pass data into NN that is of differing lengths, so need more preprocessing\n",
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
        "\n",
        "#if the review is greater than 250 words then trim off the extra words\n",
        "#if the review is less than 250 words add the necessary amount of 0's to make it equal to 250."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX6xrgn-6eGr"
      },
      "source": [
        "# create the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),  # transform the words into vectors, of 32 dimensions\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")                           \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaGMTkcT7kcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "dfcf7708-0a8f-4267-ff90-72c931d06350"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LWeuCkR7pLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "945198b2-04cd-4a08-ab7a-e3d8e9e00ed3"
      },
      "source": [
        "# compile and training the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.4198 - acc: 0.8081 - val_loss: 0.2882 - val_acc: 0.8880\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.2455 - acc: 0.9093 - val_loss: 0.3277 - val_acc: 0.8730\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.1885 - acc: 0.9311 - val_loss: 0.2811 - val_acc: 0.8852\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.1585 - acc: 0.9423 - val_loss: 0.2967 - val_acc: 0.8892\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.1308 - acc: 0.9539 - val_loss: 0.3230 - val_acc: 0.8830\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.1115 - acc: 0.9613 - val_loss: 0.3071 - val_acc: 0.8924\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0974 - acc: 0.9668 - val_loss: 0.3661 - val_acc: 0.8792\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0856 - acc: 0.9707 - val_loss: 0.3395 - val_acc: 0.8848\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0759 - acc: 0.9749 - val_loss: 0.3500 - val_acc: 0.8724\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 14s 23ms/step - loss: 0.0692 - acc: 0.9776 - val_loss: 0.3656 - val_acc: 0.8770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokBL83C8I1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ed49fe63-0d63-4ee5-fd5e-37ff6032e185"
      },
      "source": [
        "# evaluate model\n",
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 5s 6ms/step - loss: 0.4189 - acc: 0.8538\n",
            "[0.41894862055778503, 0.8537999987602234]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1lWBrUl9CTd"
      },
      "source": [
        "# making predictions\n",
        "# change new text into the encoded preprocessed data from the dataset\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C00XvzKh-Ocf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "452c2cb6-614c-476f-d467-2e3a367a90ca"
      },
      "source": [
        "# decode data from integer to words\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Sn0YvW-iww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b1fe45b7-7a7f-45c4-b31e-eaeab814b776"
      },
      "source": [
        "# make a prediction\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was awesome! Really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.28831795]\n",
            "[0.24100985]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLAFNxT-_A5A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}